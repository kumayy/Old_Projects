{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow import random\n",
    "random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = \"MNIST/digit_xtrain.csv\"\n",
    "X_TEST_PATH = \"MNIST/digit_xtest.csv\"\n",
    "Y_TRAIN_PATH = \"MNIST/digit_ytrain.csv\"\n",
    "Y_TEST_PATH = \"MNIST/digit_ytest.csv\"\n",
    "\n",
    "LOGGING_PATH = \"tensorboard_mnist_digit_logs/\"\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 1000\n",
    "TOTAL_INPUTS = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=\",\", dtype = int)\n",
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=\",\", dtype = int)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=\",\", dtype = int)\n",
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=\",\", dtype = int)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale\n",
    "x_train_all , x_test = x_train_all/255.0 , x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert target to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]\n",
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59000, 784)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape= [None, TOTAL_INPUTS], name=\"X\")\n",
    "Y = tf.compat.v1.placeholder(tf.float32, shape=[None, 10], name=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value=initial_w, name='W')\n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=initial_b, name='B')\n",
    "\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "        \n",
    "        if name=='out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "        \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        \n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_wl = tf.compat.v1.truncated_normal(shape= [TOTAL_INPUTS, n_hidden1], stddev= 0.1, seed=42)\n",
    "# w1= tf.Variable(initial_value=initial_wl)\n",
    "\n",
    "# initial_b1 = tf.constant( value= 0.0, shape= [n_hidden1])\n",
    "# b1 = tf.Variable(initial_value = initial_b1)\n",
    "\n",
    "# layer1_in = tf.matmul(X, w1) + b1\n",
    "\n",
    "# layer1_out = tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_w2 = tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], stddev= 0.1, seed=42)\n",
    "# w2 = tf.Variable(initial_value=initial_w2)\n",
    "\n",
    "# initial_b2= tf.constant(value = 0.0, shape=[n_hidden2])\n",
    "# b2 = tf.Variable(initial_value= initial_b2)\n",
    "\n",
    "# layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "\n",
    "# layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_w3 = tf.compat.v1.truncated_normal(shape=[n_hidden2, 10], stddev= 0.1, seed=42)\n",
    "# w3 = tf.Variable(initial_value=initial_w3)\n",
    "\n",
    "# initial_b3= tf.constant(value = 0.0, shape=[10])\n",
    "# b3 = tf.Variable(initial_value= initial_b3)\n",
    "\n",
    "# layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "\n",
    "# output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories\n"
     ]
    }
   ],
   "source": [
    "folder_name=f'Model 1 at{strftime(\"%H %M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print(\"Successfully created directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimisation & Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis=1, name='prediction')\n",
    "    correct_pred = tf.equal(model_prediction, tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + \"/validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_examples= y_train.shape[0]\n",
    "nr_iterations = int(nr_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "\n",
    "    global nr_examples\n",
    "    global index_in_epoch\n",
    "\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch = index_in_epoch + batch_size\n",
    "\n",
    "    if index_in_epoch > nr_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "\n",
    "    end = index_in_epoch\n",
    "\n",
    "    return data[start:end], labels[start:end]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.8619999885559082\n",
      "Epoch 1 \t| Training Accuracy = 0.8859999775886536\n",
      "Epoch 2 \t| Training Accuracy = 0.8849999904632568\n",
      "Epoch 3 \t| Training Accuracy = 0.925000011920929\n",
      "Epoch 4 \t| Training Accuracy = 0.9760000109672546\n",
      "Epoch 5 \t| Training Accuracy = 0.9769999980926514\n",
      "Epoch 6 \t| Training Accuracy = 0.9760000109672546\n",
      "Epoch 7 \t| Training Accuracy = 0.9829999804496765\n",
      "Epoch 8 \t| Training Accuracy = 0.9789999723434448\n",
      "Epoch 9 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch 10 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 11 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 12 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 13 \t| Training Accuracy = 0.9860000014305115\n",
      "Epoch 14 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 15 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 16 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 17 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 18 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 19 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 20 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 21 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 22 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 23 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 24 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 25 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 26 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 27 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 28 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 29 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 30 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 31 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 32 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 33 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 34 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 35 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 36 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 37 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 38 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 39 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 40 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 41 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 42 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 43 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 44 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 45 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 46 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 47 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 48 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 49 \t| Training Accuracy = 0.9919999837875366\n",
      "# To Train Them İs My Cause!!!.. #\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    for i in range(nr_iterations):\n",
    "\n",
    "        # =============Training Dataset=========\n",
    "\n",
    "        batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    \n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "        \n",
    "    train_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "\n",
    "    \t# ===============Validation===========\n",
    "\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary,epoch)\n",
    "\n",
    "print(\"# To Train Them İs My Cause!!!.. #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-175-1f852335f19d>:3: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From C:\\Users\\mbatu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:200: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: SavedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "outputs = {\"accuracy_calc/prediction\": model_prediction}\n",
    "inputs = {'X': X}\n",
    "tf.saved_model.simple_save(sess,\"SavedModel\", inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"MNIST/test_img.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = img.convert(\"L\")\n",
    "img_array = np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = img_array.ravel()\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sess.run(fetches=tf.argmax(output, axis=1),\n",
    " feed_dict={X:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction for test image is {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 97.89%\n"
     ]
    }
   ],
   "source": [
    "test_acc=sess.run(fetches= accuracy, feed_dict={X:x_test, Y: y_test})\n",
    "print(f\"Accuracy on test set is {test_acc:0.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset for Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1613e0379ee1b8bd526201e60bc0350032262b5e1c15a84ba45d0a319cfc4e36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
